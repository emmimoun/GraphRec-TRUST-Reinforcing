# DIFFERANCE CONDITION  5
# MODIFY TRUST CONDITION  14
epoch : 1 / 100  :  2022-05-29 05:11:15
train
[1,     0] loss: 0.080, The best rmse/mae: 9999.000000 / 9999.000000
[1,   100] loss: 5.710, The best rmse/mae: 9999.000000 / 9999.000000
test
rmse: 2.1267, mae:1.9654 
epoch : 2 / 100  :  2022-05-29 05:13:03
train
[2,     0] loss: 0.043, The best rmse/mae: 2.126656 / 1.965360
[2,   100] loss: 2.651, The best rmse/mae: 2.126656 / 1.965360
test
rmse: 1.1489, mae:0.9588 
epoch : 3 / 100  :  2022-05-29 05:14:50
train
[3,     0] loss: 0.021, The best rmse/mae: 1.148869 / 0.958833
[3,   100] loss: 1.604, The best rmse/mae: 1.148869 / 0.958833
test
rmse: 0.9767, mae:0.7748 
epoch : 4 / 100  :  2022-05-29 05:16:36
train
[4,     0] loss: 0.017, The best rmse/mae: 0.976695 / 0.774754
[4,   100] loss: 1.304, The best rmse/mae: 0.976695 / 0.774754
test
rmse: 0.8439, mae:0.6555 
epoch : 5 / 100  :  2022-05-29 05:18:22
train
[5,     0] loss: 0.011, The best rmse/mae: 0.843874 / 0.655543
[5,   100] loss: 1.118, The best rmse/mae: 0.843874 / 0.655543
test
rmse: 0.9179, mae:0.6834 
epoch : 6 / 100  :  2022-05-29 05:20:10
train
[6,     0] loss: 0.009, The best rmse/mae: 0.843874 / 0.655543
[6,   100] loss: 1.027, The best rmse/mae: 0.843874 / 0.655543
test
rmse: 0.8307, mae:0.6482 
epoch : 7 / 100  :  2022-05-29 05:21:59
train
[7,     0] loss: 0.009, The best rmse/mae: 0.830696 / 0.648169
[7,   100] loss: 0.957, The best rmse/mae: 0.830696 / 0.648169
test
rmse: 0.8692, mae:0.6816 
epoch : 8 / 100  :  2022-05-29 05:23:52
train
[8,     0] loss: 0.011, The best rmse/mae: 0.830696 / 0.648169
[8,   100] loss: 0.880, The best rmse/mae: 0.830696 / 0.648169
test
rmse: 0.8480, mae:0.6369 
epoch : 9 / 100  :  2022-05-29 05:25:43
train
[9,     0] loss: 0.009, The best rmse/mae: 0.830696 / 0.648169
[9,   100] loss: 0.869, The best rmse/mae: 0.830696 / 0.648169
test
rmse: 0.8729, mae:0.6555 
epoch : 10 / 100  :  2022-05-29 05:27:32
train
[10,     0] loss: 0.009, The best rmse/mae: 0.830696 / 0.648169
[10,   100] loss: 0.835, The best rmse/mae: 0.830696 / 0.648169
test
rmse: 0.8560, mae:0.6454 
epoch : 11 / 100  :  2022-05-29 05:29:21
train
[11,     0] loss: 0.009, The best rmse/mae: 0.830696 / 0.648169
[11,   100] loss: 0.794, The best rmse/mae: 0.830696 / 0.648169
test
rmse: 0.8146, mae:0.6329 
epoch : 12 / 100  :  2022-05-29 05:31:14
train
[12,     0] loss: 0.006, The best rmse/mae: 0.814636 / 0.632900
[12,   100] loss: 0.774, The best rmse/mae: 0.814636 / 0.632900
test
rmse: 0.8056, mae:0.6295 
epoch : 13 / 100  :  2022-05-29 05:33:04
train
[13,     0] loss: 0.006, The best rmse/mae: 0.805643 / 0.629461
[13,   100] loss: 0.752, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8320, mae:0.6285 
epoch : 14 / 100  :  2022-05-29 05:34:57
train
[14,     0] loss: 0.008, The best rmse/mae: 0.805643 / 0.629461
[14,   100] loss: 0.732, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8691, mae:0.6465 
epoch : 15 / 100  :  2022-05-29 05:36:54
train
[15,     0] loss: 0.007, The best rmse/mae: 0.805643 / 0.629461
[15,   100] loss: 0.720, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8231, mae:0.6257 
epoch : 16 / 100  :  2022-05-29 05:38:47
train
[16,     0] loss: 0.006, The best rmse/mae: 0.805643 / 0.629461
[16,   100] loss: 0.708, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8169, mae:0.6243 
epoch : 17 / 100  :  2022-05-29 05:40:41
train
[17,     0] loss: 0.007, The best rmse/mae: 0.805643 / 0.629461
[17,   100] loss: 0.701, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8129, mae:0.6299 
epoch : 18 / 100  :  2022-05-29 05:42:26
train
[18,     0] loss: 0.005, The best rmse/mae: 0.805643 / 0.629461
[18,   100] loss: 0.675, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8166, mae:0.6293 
epoch : 19 / 100  :  2022-05-29 05:44:10
train
[19,     0] loss: 0.006, The best rmse/mae: 0.805643 / 0.629461
[19,   100] loss: 0.674, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8146, mae:0.6239 
epoch : 20 / 100  :  2022-05-29 05:45:55
train
[20,     0] loss: 0.006, The best rmse/mae: 0.805643 / 0.629461
[20,   100] loss: 0.654, The best rmse/mae: 0.805643 / 0.629461
test
rmse: 0.8004, mae:0.6270 
epoch : 21 / 100  :  2022-05-29 05:47:41
train
[21,     0] loss: 0.006, The best rmse/mae: 0.800443 / 0.627031
[21,   100] loss: 0.647, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.8117, mae:0.6194 
epoch : 22 / 100  :  2022-05-29 05:49:27
train
[22,     0] loss: 0.005, The best rmse/mae: 0.800443 / 0.627031
[22,   100] loss: 0.636, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.8016, mae:0.6291 
epoch : 23 / 100  :  2022-05-29 05:51:14
train
[23,     0] loss: 0.005, The best rmse/mae: 0.800443 / 0.627031
[23,   100] loss: 0.636, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.8080, mae:0.6236 
epoch : 24 / 100  :  2022-05-29 05:53:02
train
[24,     0] loss: 0.005, The best rmse/mae: 0.800443 / 0.627031
[24,   100] loss: 0.621, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.8053, mae:0.6223 
epoch : 25 / 100  :  2022-05-29 05:54:51
train
[25,     0] loss: 0.005, The best rmse/mae: 0.800443 / 0.627031
[25,   100] loss: 0.616, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.8022, mae:0.6191 
epoch : 26 / 100  :  2022-05-29 05:56:37
train
[26,     0] loss: 0.006, The best rmse/mae: 0.800443 / 0.627031
[26,   100] loss: 0.606, The best rmse/mae: 0.800443 / 0.627031
test
rmse: 0.7987, mae:0.6231 
epoch : 27 / 100  :  2022-05-29 05:58:27
train
[27,     0] loss: 0.006, The best rmse/mae: 0.798743 / 0.623126
[27,   100] loss: 0.607, The best rmse/mae: 0.798743 / 0.623126
test
rmse: 0.7996, mae:0.6202 
epoch : 28 / 100  :  2022-05-29 06:00:18
train
[28,     0] loss: 0.006, The best rmse/mae: 0.798743 / 0.623126
[28,   100] loss: 0.606, The best rmse/mae: 0.798743 / 0.623126
test
rmse: 0.7990, mae:0.6205 
epoch : 29 / 100  :  2022-05-29 06:02:07
train
[29,     0] loss: 0.005, The best rmse/mae: 0.798743 / 0.623126
[29,   100] loss: 0.604, The best rmse/mae: 0.798743 / 0.623126
test
rmse: 0.8230, mae:0.6508 
epoch : 30 / 100  :  2022-05-29 06:03:56
train
[30,     0] loss: 0.006, The best rmse/mae: 0.798743 / 0.623126
[30,   100] loss: 0.601, The best rmse/mae: 0.798743 / 0.623126
test
rmse: 0.8017, mae:0.6222 
epoch : 31 / 100  :  2022-05-29 06:05:45
train
[31,     0] loss: 0.006, The best rmse/mae: 0.798743 / 0.623126
[31,   100] loss: 0.598, The best rmse/mae: 0.798743 / 0.623126
test
rmse: 0.7978, mae:0.6220 
epoch : 32 / 100  :  2022-05-29 06:07:35
train
[32,     0] loss: 0.006, The best rmse/mae: 0.797835 / 0.622000
[32,   100] loss: 0.597, The best rmse/mae: 0.797835 / 0.622000
test
rmse: 0.8143, mae:0.6501 
epoch : 33 / 100  :  2022-05-29 06:09:25
train
[33,     0] loss: 0.005, The best rmse/mae: 0.797835 / 0.622000
[33,   100] loss: 0.594, The best rmse/mae: 0.797835 / 0.622000
test
rmse: 0.8000, mae:0.6248 
epoch : 34 / 100  :  2022-05-29 06:11:15
train
[34,     0] loss: 0.005, The best rmse/mae: 0.797835 / 0.622000
[34,   100] loss: 0.586, The best rmse/mae: 0.797835 / 0.622000
test
rmse: 0.8081, mae:0.6392 
epoch : 35 / 100  :  2022-05-29 06:13:04
train
[35,     0] loss: 0.006, The best rmse/mae: 0.797835 / 0.622000
[35,   100] loss: 0.581, The best rmse/mae: 0.797835 / 0.622000
test
rmse: 0.7986, mae:0.6210 
epoch : 36 / 100  :  2022-05-29 06:14:52
train
[36,     0] loss: 0.005, The best rmse/mae: 0.797835 / 0.622000
[36,   100] loss: 0.599, The best rmse/mae: 0.797835 / 0.622000
test
rmse: 0.7975, mae:0.6230 
epoch : 37 / 100  :  2022-05-29 06:16:42
train
[37,     0] loss: 0.006, The best rmse/mae: 0.797515 / 0.623029
[37,   100] loss: 0.581, The best rmse/mae: 0.797515 / 0.623029
test
rmse: 0.8004, mae:0.6163 
epoch : 38 / 100  :  2022-05-29 06:18:34
train
[38,     0] loss: 0.006, The best rmse/mae: 0.797515 / 0.623029
[38,   100] loss: 0.585, The best rmse/mae: 0.797515 / 0.623029
test
rmse: 0.8142, mae:0.6300 
epoch : 39 / 100  :  2022-05-29 06:20:22
train
[39,     0] loss: 0.006, The best rmse/mae: 0.797515 / 0.623029
[39,   100] loss: 0.582, The best rmse/mae: 0.797515 / 0.623029
test
rmse: 0.7969, mae:0.6268 
epoch : 40 / 100  :  2022-05-29 06:22:13
train
[40,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[40,   100] loss: 0.587, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8025, mae:0.6310 
epoch : 41 / 100  :  2022-05-29 06:24:02
train
[41,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[41,   100] loss: 0.583, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8178, mae:0.6558 
epoch : 42 / 100  :  2022-05-29 06:25:51
train
[42,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[42,   100] loss: 0.583, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8079, mae:0.6213 
epoch : 43 / 100  :  2022-05-29 06:27:41
train
[43,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[43,   100] loss: 0.578, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8114, mae:0.6325 
epoch : 44 / 100  :  2022-05-29 06:29:41
train
[44,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[44,   100] loss: 0.583, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8006, mae:0.6336 
epoch : 45 / 100  :  2022-05-29 06:31:39
train
[45,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[45,   100] loss: 0.576, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8166, mae:0.6481 
epoch : 46 / 100  :  2022-05-29 06:33:34
train
[46,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[46,   100] loss: 0.578, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8253, mae:0.6545 
epoch : 47 / 100  :  2022-05-29 06:35:26
train
[47,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[47,   100] loss: 0.572, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8055, mae:0.6209 
epoch : 48 / 100  :  2022-05-29 06:37:20
train
[48,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[48,   100] loss: 0.579, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8327, mae:0.6316 
epoch : 49 / 100  :  2022-05-29 06:39:11
train
[49,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[49,   100] loss: 0.576, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8063, mae:0.6296 
epoch : 50 / 100  :  2022-05-29 06:41:02
train
[50,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[50,   100] loss: 0.576, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8076, mae:0.6230 
epoch : 51 / 100  :  2022-05-29 06:42:55
train
[51,     0] loss: 0.007, The best rmse/mae: 0.796875 / 0.626798
[51,   100] loss: 0.569, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8299, mae:0.6662 
epoch : 52 / 100  :  2022-05-29 06:44:47
train
[52,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[52,   100] loss: 0.576, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8044, mae:0.6270 
epoch : 53 / 100  :  2022-05-29 06:46:43
train
[53,     0] loss: 0.004, The best rmse/mae: 0.796875 / 0.626798
[53,   100] loss: 0.579, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8172, mae:0.6433 
epoch : 54 / 100  :  2022-05-29 06:48:32
train
[54,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[54,   100] loss: 0.577, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8171, mae:0.6380 
epoch : 55 / 100  :  2022-05-29 06:50:17
train
[55,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[55,   100] loss: 0.570, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8203, mae:0.6540 
epoch : 56 / 100  :  2022-05-29 06:52:01
train
[56,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[56,   100] loss: 0.570, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8063, mae:0.6371 
epoch : 57 / 100  :  2022-05-29 06:53:48
train
[57,     0] loss: 0.007, The best rmse/mae: 0.796875 / 0.626798
[57,   100] loss: 0.567, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8479, mae:0.6820 
epoch : 58 / 100  :  2022-05-29 06:55:32
train
[58,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[58,   100] loss: 0.571, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8159, mae:0.6449 
epoch : 59 / 100  :  2022-05-29 06:57:24
train
[59,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[59,   100] loss: 0.573, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8059, mae:0.6327 
epoch : 60 / 100  :  2022-05-29 06:59:10
train
[60,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[60,   100] loss: 0.567, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8072, mae:0.6230 
epoch : 61 / 100  :  2022-05-29 07:00:58
train
[61,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[61,   100] loss: 0.573, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8050, mae:0.6216 
epoch : 62 / 100  :  2022-05-29 07:02:44
train
[62,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[62,   100] loss: 0.569, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8065, mae:0.6290 
epoch : 63 / 100  :  2022-05-29 07:04:42
train
[63,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[63,   100] loss: 0.570, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8004, mae:0.6206 
epoch : 64 / 100  :  2022-05-29 07:06:40
train
[64,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[64,   100] loss: 0.564, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8323, mae:0.6664 
epoch : 65 / 100  :  2022-05-29 07:08:33
train
[65,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[65,   100] loss: 0.566, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8117, mae:0.6281 
epoch : 66 / 100  :  2022-05-29 07:10:26
train
[66,     0] loss: 0.004, The best rmse/mae: 0.796875 / 0.626798
[66,   100] loss: 0.568, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8176, mae:0.6471 
epoch : 67 / 100  :  2022-05-29 07:12:12
train
[67,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[67,   100] loss: 0.573, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8231, mae:0.6433 
epoch : 68 / 100  :  2022-05-29 07:13:55
train
[68,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[68,   100] loss: 0.570, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8042, mae:0.6206 
epoch : 69 / 100  :  2022-05-29 07:15:40
train
[69,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[69,   100] loss: 0.566, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8169, mae:0.6451 
epoch : 70 / 100  :  2022-05-29 07:17:25
train
[70,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[70,   100] loss: 0.569, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8266, mae:0.6290 
epoch : 71 / 100  :  2022-05-29 07:19:09
train
[71,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[71,   100] loss: 0.565, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8234, mae:0.6494 
epoch : 72 / 100  :  2022-05-29 07:20:54
train
[72,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[72,   100] loss: 0.568, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8029, mae:0.6311 
epoch : 73 / 100  :  2022-05-29 07:22:37
train
[73,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[73,   100] loss: 0.566, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8111, mae:0.6369 
epoch : 74 / 100  :  2022-05-29 07:24:21
train
[74,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[74,   100] loss: 0.571, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8044, mae:0.6301 
epoch : 75 / 100  :  2022-05-29 07:26:05
train
[75,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[75,   100] loss: 0.571, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8091, mae:0.6358 
epoch : 76 / 100  :  2022-05-29 07:27:49
train
[76,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[76,   100] loss: 0.560, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8107, mae:0.6379 
epoch : 77 / 100  :  2022-05-29 07:29:33
train
[77,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[77,   100] loss: 0.566, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8120, mae:0.6311 
epoch : 78 / 100  :  2022-05-29 07:31:17
train
[78,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[78,   100] loss: 0.563, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8077, mae:0.6228 
epoch : 79 / 100  :  2022-05-29 07:33:01
train
[79,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[79,   100] loss: 0.561, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8347, mae:0.6723 
epoch : 80 / 100  :  2022-05-29 07:34:44
train
[80,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[80,   100] loss: 0.561, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8193, mae:0.6441 
epoch : 81 / 100  :  2022-05-29 07:36:28
train
[81,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[81,   100] loss: 0.565, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8027, mae:0.6285 
epoch : 82 / 100  :  2022-05-29 07:38:12
train
[82,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[82,   100] loss: 0.563, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8073, mae:0.6394 
epoch : 83 / 100  :  2022-05-29 07:39:55
train
[83,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[83,   100] loss: 0.557, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8201, mae:0.6412 
epoch : 84 / 100  :  2022-05-29 07:41:39
train
[84,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[84,   100] loss: 0.563, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8117, mae:0.6238 
epoch : 85 / 100  :  2022-05-29 07:43:23
train
[85,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[85,   100] loss: 0.562, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8103, mae:0.6233 
epoch : 86 / 100  :  2022-05-29 07:45:07
train
[86,     0] loss: 0.004, The best rmse/mae: 0.796875 / 0.626798
[86,   100] loss: 0.567, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8055, mae:0.6248 
epoch : 87 / 100  :  2022-05-29 07:46:52
train
[87,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[87,   100] loss: 0.565, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8071, mae:0.6227 
epoch : 88 / 100  :  2022-05-29 07:48:36
train
[88,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[88,   100] loss: 0.560, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8064, mae:0.6237 
epoch : 89 / 100  :  2022-05-29 07:50:27
train
[89,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[89,   100] loss: 0.560, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8096, mae:0.6380 
epoch : 90 / 100  :  2022-05-29 07:52:12
train
[90,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[90,   100] loss: 0.560, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8201, mae:0.6530 
epoch : 91 / 100  :  2022-05-29 07:53:55
train
[91,     0] loss: 0.007, The best rmse/mae: 0.796875 / 0.626798
[91,   100] loss: 0.566, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8046, mae:0.6385 
epoch : 92 / 100  :  2022-05-29 07:55:39
train
[92,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[92,   100] loss: 0.559, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8234, mae:0.6553 
epoch : 93 / 100  :  2022-05-29 07:57:24
train
[93,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[93,   100] loss: 0.567, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8078, mae:0.6365 
epoch : 94 / 100  :  2022-05-29 07:59:08
train
[94,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[94,   100] loss: 0.561, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8155, mae:0.6364 
epoch : 95 / 100  :  2022-05-29 08:00:51
train
[95,     0] loss: 0.007, The best rmse/mae: 0.796875 / 0.626798
[95,   100] loss: 0.562, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8038, mae:0.6236 
epoch : 96 / 100  :  2022-05-29 08:02:35
train
[96,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[96,   100] loss: 0.564, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8160, mae:0.6528 
epoch : 97 / 100  :  2022-05-29 08:04:18
train
[97,     0] loss: 0.005, The best rmse/mae: 0.796875 / 0.626798
[97,   100] loss: 0.565, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8136, mae:0.6432 
epoch : 98 / 100  :  2022-05-29 08:06:02
train
[98,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[98,   100] loss: 0.559, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8261, mae:0.6581 
epoch : 99 / 100  :  2022-05-29 08:07:45
train
[99,     0] loss: 0.006, The best rmse/mae: 0.796875 / 0.626798
[99,   100] loss: 0.564, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8195, mae:0.6521 
epoch : 100 / 100  :  2022-05-29 08:09:29
train
[100,     0] loss: 0.007, The best rmse/mae: 0.796875 / 0.626798
[100,   100] loss: 0.559, The best rmse/mae: 0.796875 / 0.626798
test
rmse: 0.8015, mae:0.6266 


########################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################
#######################################################

# DIFFERANCE CONDITION  2
# MODIFY TRUST CONDITION  12
epoch : 1 / 100  :  2022-05-28 02:21:38
train
[1,     0] loss: 0.122, The best rmse/mae: 9999.000000 / 9999.000000
[1,   100] loss: 8.596, The best rmse/mae: 9999.000000 / 9999.000000
test
rmse: 2.4685, mae:2.3130 
epoch : 2 / 100  :  2022-05-28 02:23:25
train
[2,     0] loss: 0.058, The best rmse/mae: 2.468528 / 2.312992
[2,   100] loss: 4.205, The best rmse/mae: 2.468528 / 2.312992
test
rmse: 1.7880, mae:1.6005 
epoch : 3 / 100  :  2022-05-28 02:25:16
train
[3,     0] loss: 0.027, The best rmse/mae: 1.787965 / 1.600520
[3,   100] loss: 2.068, The best rmse/mae: 1.787965 / 1.600520
test
rmse: 1.0349, mae:0.8384 
epoch : 4 / 100  :  2022-05-28 02:27:05
train
[4,     0] loss: 0.017, The best rmse/mae: 1.034880 / 0.838390
[4,   100] loss: 1.527, The best rmse/mae: 1.034880 / 0.838390
test
rmse: 0.8721, mae:0.6809 
epoch : 5 / 100  :  2022-05-28 02:28:53
train
[5,     0] loss: 0.014, The best rmse/mae: 0.872104 / 0.680876
[5,   100] loss: 1.266, The best rmse/mae: 0.872104 / 0.680876
test
rmse: 0.8320, mae:0.6542 
epoch : 6 / 100  :  2022-05-28 02:30:38
train
[6,     0] loss: 0.012, The best rmse/mae: 0.832047 / 0.654245
[6,   100] loss: 1.108, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 1.0972, mae:0.8177 
epoch : 7 / 100  :  2022-05-28 02:32:23
train
[7,     0] loss: 0.010, The best rmse/mae: 0.832047 / 0.654245
[7,   100] loss: 1.028, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 0.8763, mae:0.6551 
epoch : 8 / 100  :  2022-05-28 02:34:05
train
[8,     0] loss: 0.008, The best rmse/mae: 0.832047 / 0.654245
[8,   100] loss: 0.963, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 0.8408, mae:0.6351 
epoch : 9 / 100  :  2022-05-28 02:35:54
train
[9,     0] loss: 0.009, The best rmse/mae: 0.832047 / 0.654245
[9,   100] loss: 0.898, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 0.9434, mae:0.7069 
epoch : 10 / 100  :  2022-05-28 02:37:44
train
[10,     0] loss: 0.010, The best rmse/mae: 0.832047 / 0.654245
[10,   100] loss: 0.892, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 0.8967, mae:0.6721 
epoch : 11 / 100  :  2022-05-28 02:39:33
train
[11,     0] loss: 0.009, The best rmse/mae: 0.832047 / 0.654245
[11,   100] loss: 0.871, The best rmse/mae: 0.832047 / 0.654245
test
rmse: 0.8238, mae:0.6405 
epoch : 12 / 100  :  2022-05-28 02:41:25
train
[12,     0] loss: 0.010, The best rmse/mae: 0.823827 / 0.640457
[12,   100] loss: 0.832, The best rmse/mae: 0.823827 / 0.640457
test
rmse: 0.8200, mae:0.6312 
epoch : 13 / 100  :  2022-05-28 02:43:16
train
[13,     0] loss: 0.008, The best rmse/mae: 0.819966 / 0.631249
[13,   100] loss: 0.800, The best rmse/mae: 0.819966 / 0.631249
test
rmse: 0.8180, mae:0.6310 
epoch : 14 / 100  :  2022-05-28 02:45:01
train
[14,     0] loss: 0.007, The best rmse/mae: 0.818020 / 0.631003
[14,   100] loss: 0.800, The best rmse/mae: 0.818020 / 0.631003
test
rmse: 0.8156, mae:0.6259 
epoch : 15 / 100  :  2022-05-28 02:46:47
train
[15,     0] loss: 0.007, The best rmse/mae: 0.815635 / 0.625923
[15,   100] loss: 0.770, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8303, mae:0.6291 
epoch : 16 / 100  :  2022-05-28 02:49:05
train
[16,     0] loss: 0.007, The best rmse/mae: 0.815635 / 0.625923
[16,   100] loss: 0.752, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8269, mae:0.6283 
epoch : 17 / 100  :  2022-05-28 02:51:29
train
[17,     0] loss: 0.007, The best rmse/mae: 0.815635 / 0.625923
[17,   100] loss: 0.727, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8178, mae:0.6342 
epoch : 18 / 100  :  2022-05-28 02:53:45
train
[18,     0] loss: 0.007, The best rmse/mae: 0.815635 / 0.625923
[18,   100] loss: 0.727, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8160, mae:0.6491 
epoch : 19 / 100  :  2022-05-28 02:55:59
train
[19,     0] loss: 0.008, The best rmse/mae: 0.815635 / 0.625923
[19,   100] loss: 0.700, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8295, mae:0.6302 
epoch : 20 / 100  :  2022-05-28 02:58:16
train
[20,     0] loss: 0.008, The best rmse/mae: 0.815635 / 0.625923
[20,   100] loss: 0.697, The best rmse/mae: 0.815635 / 0.625923
test
rmse: 0.8129, mae:0.6410 
epoch : 21 / 100  :  2022-05-28 03:00:32
train
[21,     0] loss: 0.006, The best rmse/mae: 0.812896 / 0.640961
[21,   100] loss: 0.674, The best rmse/mae: 0.812896 / 0.640961
test
rmse: 0.8159, mae:0.6219 
epoch : 22 / 100  :  2022-05-28 03:02:56
train
[22,     0] loss: 0.008, The best rmse/mae: 0.812896 / 0.640961
[22,   100] loss: 0.660, The best rmse/mae: 0.812896 / 0.640961
test
rmse: 0.8076, mae:0.6184 
epoch : 23 / 100  :  2022-05-28 03:05:06
train
[23,     0] loss: 0.007, The best rmse/mae: 0.807593 / 0.618389
[23,   100] loss: 0.657, The best rmse/mae: 0.807593 / 0.618389
test
rmse: 0.8033, mae:0.6145 
epoch : 24 / 100  :  2022-05-28 03:07:15
train
[24,     0] loss: 0.007, The best rmse/mae: 0.803278 / 0.614518
[24,   100] loss: 0.637, The best rmse/mae: 0.803278 / 0.614518
test
rmse: 0.8392, mae:0.6571 
epoch : 25 / 100  :  2022-05-28 03:09:26
train
[25,     0] loss: 0.005, The best rmse/mae: 0.803278 / 0.614518
[25,   100] loss: 0.642, The best rmse/mae: 0.803278 / 0.614518
test
rmse: 0.8171, mae:0.6335 
epoch : 26 / 100  :  2022-05-28 03:11:36
train
[26,     0] loss: 0.006, The best rmse/mae: 0.803278 / 0.614518
[26,   100] loss: 0.633, The best rmse/mae: 0.803278 / 0.614518
test
rmse: 0.8091, mae:0.6331 
epoch : 27 / 100  :  2022-05-28 03:13:45
train
[27,     0] loss: 0.005, The best rmse/mae: 0.803278 / 0.614518
[27,   100] loss: 0.636, The best rmse/mae: 0.803278 / 0.614518
test
rmse: 0.8019, mae:0.6345 
epoch : 28 / 100  :  2022-05-28 03:15:52
train
[28,     0] loss: 0.005, The best rmse/mae: 0.801931 / 0.634537
[28,   100] loss: 0.633, The best rmse/mae: 0.801931 / 0.634537
test
rmse: 0.8036, mae:0.6340 
epoch : 29 / 100  :  2022-05-28 03:18:03
train
[29,     0] loss: 0.006, The best rmse/mae: 0.801931 / 0.634537
[29,   100] loss: 0.628, The best rmse/mae: 0.801931 / 0.634537
test
rmse: 0.8068, mae:0.6250 
epoch : 30 / 100  :  2022-05-28 03:20:11
train
[30,     0] loss: 0.007, The best rmse/mae: 0.801931 / 0.634537
[30,   100] loss: 0.619, The best rmse/mae: 0.801931 / 0.634537
test
rmse: 0.8362, mae:0.6645 
epoch : 31 / 100  :  2022-05-28 03:22:16
train
[31,     0] loss: 0.007, The best rmse/mae: 0.801931 / 0.634537
[31,   100] loss: 0.624, The best rmse/mae: 0.801931 / 0.634537
test
rmse: 0.8276, mae:0.6542 
epoch : 32 / 100  :  2022-05-28 03:24:20
train
[32,     0] loss: 0.007, The best rmse/mae: 0.801931 / 0.634537
[32,   100] loss: 0.605, The best rmse/mae: 0.801931 / 0.634537
test
rmse: 0.7995, mae:0.6246 
epoch : 33 / 100  :  2022-05-28 03:26:22
train
[33,     0] loss: 0.007, The best rmse/mae: 0.799461 / 0.624613
[33,   100] loss: 0.612, The best rmse/mae: 0.799461 / 0.624613
test
rmse: 0.8001, mae:0.6227 
epoch : 34 / 100  :  2022-05-28 03:28:22
train
[34,     0] loss: 0.006, The best rmse/mae: 0.799461 / 0.624613
[34,   100] loss: 0.607, The best rmse/mae: 0.799461 / 0.624613
test
rmse: 0.8026, mae:0.6328 
epoch : 35 / 100  :  2022-05-28 03:30:25
train
[35,     0] loss: 0.007, The best rmse/mae: 0.799461 / 0.624613
[35,   100] loss: 0.610, The best rmse/mae: 0.799461 / 0.624613
test
rmse: 0.7967, mae:0.6277 
epoch : 36 / 100  :  2022-05-28 03:32:28
train
[36,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[36,   100] loss: 0.606, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8030, mae:0.6189 
epoch : 37 / 100  :  2022-05-28 03:34:32
train
[37,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[37,   100] loss: 0.607, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7969, mae:0.6249 
epoch : 38 / 100  :  2022-05-28 03:36:37
train
[38,     0] loss: 0.005, The best rmse/mae: 0.796746 / 0.627677
[38,   100] loss: 0.606, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8039, mae:0.6325 
epoch : 39 / 100  :  2022-05-28 03:38:43
train
[39,     0] loss: 0.005, The best rmse/mae: 0.796746 / 0.627677
[39,   100] loss: 0.604, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7969, mae:0.6188 
epoch : 40 / 100  :  2022-05-28 03:40:49
train
[40,     0] loss: 0.007, The best rmse/mae: 0.796746 / 0.627677
[40,   100] loss: 0.601, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8157, mae:0.6430 
epoch : 41 / 100  :  2022-05-28 03:42:55
train
[41,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[41,   100] loss: 0.596, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7971, mae:0.6204 
epoch : 42 / 100  :  2022-05-28 03:45:09
train
[42,     0] loss: 0.007, The best rmse/mae: 0.796746 / 0.627677
[42,   100] loss: 0.596, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7992, mae:0.6220 
epoch : 43 / 100  :  2022-05-28 03:47:16
train
[43,     0] loss: 0.005, The best rmse/mae: 0.796746 / 0.627677
[43,   100] loss: 0.602, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8250, mae:0.6661 
epoch : 44 / 100  :  2022-05-28 03:49:22
train
[44,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[44,   100] loss: 0.599, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8019, mae:0.6263 
epoch : 45 / 100  :  2022-05-28 03:51:27
train
[45,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[45,   100] loss: 0.602, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8067, mae:0.6431 
epoch : 46 / 100  :  2022-05-28 03:53:33
train
[46,     0] loss: 0.007, The best rmse/mae: 0.796746 / 0.627677
[46,   100] loss: 0.596, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8123, mae:0.6376 
epoch : 47 / 100  :  2022-05-28 03:55:41
train
[47,     0] loss: 0.007, The best rmse/mae: 0.796746 / 0.627677
[47,   100] loss: 0.591, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8095, mae:0.6492 
epoch : 48 / 100  :  2022-05-28 03:57:45
train
[48,     0] loss: 0.006, The best rmse/mae: 0.796746 / 0.627677
[48,   100] loss: 0.593, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7989, mae:0.6246 
epoch : 49 / 100  :  2022-05-28 03:59:52
train
[49,     0] loss: 0.005, The best rmse/mae: 0.796746 / 0.627677
[49,   100] loss: 0.596, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.8022, mae:0.6259 
epoch : 50 / 100  :  2022-05-28 04:01:57
train
[50,     0] loss: 0.004, The best rmse/mae: 0.796746 / 0.627677
[50,   100] loss: 0.594, The best rmse/mae: 0.796746 / 0.627677
test
rmse: 0.7956, mae:0.6202 
epoch : 51 / 100  :  2022-05-28 04:04:04
train
[51,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[51,   100] loss: 0.600, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8094, mae:0.6344 
epoch : 52 / 100  :  2022-05-28 04:06:08
train
[52,     0] loss: 0.006, The best rmse/mae: 0.795552 / 0.620164
[52,   100] loss: 0.594, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8152, mae:0.6508 
epoch : 53 / 100  :  2022-05-28 04:08:14
train
[53,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[53,   100] loss: 0.592, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8037, mae:0.6357 
epoch : 54 / 100  :  2022-05-28 04:10:19
train
[54,     0] loss: 0.006, The best rmse/mae: 0.795552 / 0.620164
[54,   100] loss: 0.593, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8050, mae:0.6249 
epoch : 55 / 100  :  2022-05-28 04:12:25
train
[55,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[55,   100] loss: 0.593, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.7956, mae:0.6233 
epoch : 56 / 100  :  2022-05-28 04:14:30
train
[56,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[56,   100] loss: 0.592, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.7991, mae:0.6306 
epoch : 57 / 100  :  2022-05-28 04:16:35
train
[57,     0] loss: 0.006, The best rmse/mae: 0.795552 / 0.620164
[57,   100] loss: 0.586, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8027, mae:0.6205 
epoch : 58 / 100  :  2022-05-28 04:18:39
train
[58,     0] loss: 0.006, The best rmse/mae: 0.795552 / 0.620164
[58,   100] loss: 0.587, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.7987, mae:0.6312 
epoch : 59 / 100  :  2022-05-28 04:20:44
train
[59,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[59,   100] loss: 0.589, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8022, mae:0.6281 
epoch : 60 / 100  :  2022-05-28 04:22:49
train
[60,     0] loss: 0.007, The best rmse/mae: 0.795552 / 0.620164
[60,   100] loss: 0.593, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8077, mae:0.6182 
epoch : 61 / 100  :  2022-05-28 04:24:52
train
[61,     0] loss: 0.006, The best rmse/mae: 0.795552 / 0.620164
[61,   100] loss: 0.585, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.8057, mae:0.6325 
epoch : 62 / 100  :  2022-05-28 04:26:58
train
[62,     0] loss: 0.005, The best rmse/mae: 0.795552 / 0.620164
[62,   100] loss: 0.588, The best rmse/mae: 0.795552 / 0.620164
test
rmse: 0.7955, mae:0.6267 
epoch : 63 / 100  :  2022-05-28 04:29:04
train
[63,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[63,   100] loss: 0.585, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8010, mae:0.6326 
epoch : 64 / 100  :  2022-05-28 04:31:08
train
[64,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[64,   100] loss: 0.582, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7980, mae:0.6318 
epoch : 65 / 100  :  2022-05-28 04:33:14
train
[65,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[65,   100] loss: 0.587, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8275, mae:0.6602 
epoch : 66 / 100  :  2022-05-28 04:35:17
train
[66,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[66,   100] loss: 0.583, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8053, mae:0.6211 
epoch : 67 / 100  :  2022-05-28 04:37:21
train
[67,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[67,   100] loss: 0.580, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8083, mae:0.6172 
epoch : 68 / 100  :  2022-05-28 04:39:25
train
[68,     0] loss: 0.004, The best rmse/mae: 0.795503 / 0.626719
[68,   100] loss: 0.576, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7995, mae:0.6238 
epoch : 69 / 100  :  2022-05-28 04:41:30
train
[69,     0] loss: 0.007, The best rmse/mae: 0.795503 / 0.626719
[69,   100] loss: 0.573, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8206, mae:0.6665 
epoch : 70 / 100  :  2022-05-28 04:43:35
train
[70,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[70,   100] loss: 0.573, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7995, mae:0.6271 
epoch : 71 / 100  :  2022-05-28 04:45:40
train
[71,     0] loss: 0.004, The best rmse/mae: 0.795503 / 0.626719
[71,   100] loss: 0.579, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8622, mae:0.7019 
epoch : 72 / 100  :  2022-05-28 04:47:46
train
[72,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[72,   100] loss: 0.578, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8000, mae:0.6305 
epoch : 73 / 100  :  2022-05-28 04:49:50
train
[73,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[73,   100] loss: 0.576, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8074, mae:0.6410 
epoch : 74 / 100  :  2022-05-28 04:51:53
train
[74,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[74,   100] loss: 0.569, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8036, mae:0.6226 
epoch : 75 / 100  :  2022-05-28 04:53:57
train
[75,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[75,   100] loss: 0.577, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8106, mae:0.6476 
epoch : 76 / 100  :  2022-05-28 04:56:02
train
[76,     0] loss: 0.007, The best rmse/mae: 0.795503 / 0.626719
[76,   100] loss: 0.573, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8000, mae:0.6350 
epoch : 77 / 100  :  2022-05-28 04:58:07
train
[77,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[77,   100] loss: 0.579, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7989, mae:0.6315 
epoch : 78 / 100  :  2022-05-28 05:00:12
train
[78,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[78,   100] loss: 0.576, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8030, mae:0.6381 
epoch : 79 / 100  :  2022-05-28 05:02:16
train
[79,     0] loss: 0.007, The best rmse/mae: 0.795503 / 0.626719
[79,   100] loss: 0.570, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8083, mae:0.6310 
epoch : 80 / 100  :  2022-05-28 05:04:20
train
[80,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[80,   100] loss: 0.561, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8026, mae:0.6240 
epoch : 81 / 100  :  2022-05-28 05:06:26
train
[81,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[81,   100] loss: 0.576, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8082, mae:0.6205 
epoch : 82 / 100  :  2022-05-28 05:08:30
train
[82,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[82,   100] loss: 0.574, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7973, mae:0.6253 
epoch : 83 / 100  :  2022-05-28 05:10:34
train
[83,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[83,   100] loss: 0.574, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8323, mae:0.6631 
epoch : 84 / 100  :  2022-05-28 05:12:41
train
[84,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[84,   100] loss: 0.576, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8052, mae:0.6349 
epoch : 85 / 100  :  2022-05-28 05:14:45
train
[85,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[85,   100] loss: 0.570, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8085, mae:0.6342 
epoch : 86 / 100  :  2022-05-28 05:16:50
train
[86,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[86,   100] loss: 0.571, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8186, mae:0.6527 
epoch : 87 / 100  :  2022-05-28 05:18:56
train
[87,     0] loss: 0.008, The best rmse/mae: 0.795503 / 0.626719
[87,   100] loss: 0.570, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8050, mae:0.6409 
epoch : 88 / 100  :  2022-05-28 05:21:01
train
[88,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[88,   100] loss: 0.571, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7979, mae:0.6289 
epoch : 89 / 100  :  2022-05-28 05:23:06
train
[89,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[89,   100] loss: 0.569, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8053, mae:0.6320 
epoch : 90 / 100  :  2022-05-28 05:25:11
train
[90,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[90,   100] loss: 0.571, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8090, mae:0.6474 
epoch : 91 / 100  :  2022-05-28 05:27:16
train
[91,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[91,   100] loss: 0.566, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8161, mae:0.6445 
epoch : 92 / 100  :  2022-05-28 05:29:19
train
[92,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[92,   100] loss: 0.562, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.7978, mae:0.6232 
epoch : 93 / 100  :  2022-05-28 05:31:25
train
[93,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[93,   100] loss: 0.568, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8032, mae:0.6296 
epoch : 94 / 100  :  2022-05-28 05:33:31
train
[94,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[94,   100] loss: 0.567, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8176, mae:0.6576 
epoch : 95 / 100  :  2022-05-28 05:35:36
train
[95,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[95,   100] loss: 0.567, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8131, mae:0.6528 
epoch : 96 / 100  :  2022-05-28 05:37:42
train
[96,     0] loss: 0.006, The best rmse/mae: 0.795503 / 0.626719
[96,   100] loss: 0.565, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8067, mae:0.6330 
epoch : 97 / 100  :  2022-05-28 05:39:47
train
[97,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[97,   100] loss: 0.574, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8179, mae:0.6504 
epoch : 98 / 100  :  2022-05-28 05:41:54
train
[98,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[98,   100] loss: 0.565, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8042, mae:0.6423 
epoch : 99 / 100  :  2022-05-28 05:44:00
train
[99,     0] loss: 0.005, The best rmse/mae: 0.795503 / 0.626719
[99,   100] loss: 0.563, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8068, mae:0.6300 
epoch : 100 / 100  :  2022-05-28 05:46:06
train
[100,     0] loss: 0.004, The best rmse/mae: 0.795503 / 0.626719
[100,   100] loss: 0.565, The best rmse/mae: 0.795503 / 0.626719
test
rmse: 0.8075, mae:0.6378




###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################
###########################################




# DIFFERANCE CONDITION  2
# MODIFY TRUST CONDITION  6
epoch : 1 / 100  :  2022-05-28 15:19:07
train
[1,     0] loss: 0.104, The best rmse/mae: 9999.000000 / 9999.000000
[1,   100] loss: 7.633, The best rmse/mae: 9999.000000 / 9999.000000
test
rmse: 2.1526, mae:1.9829 
epoch : 2 / 100  :  2022-05-28 15:20:55
train
[2,     0] loss: 0.050, The best rmse/mae: 2.152559 / 1.982929
[2,   100] loss: 3.681, The best rmse/mae: 2.152559 / 1.982929
test
rmse: 1.6143, mae:1.4129 
epoch : 3 / 100  :  2022-05-28 15:22:49
train
[3,     0] loss: 0.026, The best rmse/mae: 1.614255 / 1.412850
[3,   100] loss: 2.007, The best rmse/mae: 1.614255 / 1.412850
test
rmse: 0.9857, mae:0.7964 
epoch : 4 / 100  :  2022-05-28 15:24:54
train
[4,     0] loss: 0.015, The best rmse/mae: 0.985704 / 0.796362
[4,   100] loss: 1.465, The best rmse/mae: 0.985704 / 0.796362
test
rmse: 0.8562, mae:0.6721 
epoch : 5 / 100  :  2022-05-28 15:27:02
train
[5,     0] loss: 0.016, The best rmse/mae: 0.856199 / 0.672079
[5,   100] loss: 1.262, The best rmse/mae: 0.856199 / 0.672079
test
rmse: 0.8393, mae:0.6674 
epoch : 6 / 100  :  2022-05-28 15:29:00
train
[6,     0] loss: 0.010, The best rmse/mae: 0.839276 / 0.667395
[6,   100] loss: 1.123, The best rmse/mae: 0.839276 / 0.667395
test
rmse: 0.8349, mae:0.6700 
epoch : 7 / 100  :  2022-05-28 15:31:08
train
[7,     0] loss: 0.012, The best rmse/mae: 0.834909 / 0.669984
[7,   100] loss: 1.080, The best rmse/mae: 0.834909 / 0.669984
test
rmse: 0.8290, mae:0.6572 
epoch : 8 / 100  :  2022-05-28 15:33:10
train
[8,     0] loss: 0.011, The best rmse/mae: 0.829003 / 0.657179
[8,   100] loss: 1.004, The best rmse/mae: 0.829003 / 0.657179
test
rmse: 0.8457, mae:0.6352 
epoch : 9 / 100  :  2022-05-28 15:35:25
train
[9,     0] loss: 0.010, The best rmse/mae: 0.829003 / 0.657179
[9,   100] loss: 0.955, The best rmse/mae: 0.829003 / 0.657179
test
rmse: 0.8106, mae:0.6280 
epoch : 10 / 100  :  2022-05-28 15:37:32
train
[10,     0] loss: 0.009, The best rmse/mae: 0.810643 / 0.627984
[10,   100] loss: 0.894, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8180, mae:0.6291 
epoch : 11 / 100  :  2022-05-28 15:39:36
train
[11,     0] loss: 0.009, The best rmse/mae: 0.810643 / 0.627984
[11,   100] loss: 0.879, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8445, mae:0.6391 
epoch : 12 / 100  :  2022-05-28 15:41:36
train
[12,     0] loss: 0.008, The best rmse/mae: 0.810643 / 0.627984
[12,   100] loss: 0.839, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8248, mae:0.6278 
epoch : 13 / 100  :  2022-05-28 15:43:36
train
[13,     0] loss: 0.007, The best rmse/mae: 0.810643 / 0.627984
[13,   100] loss: 0.799, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8540, mae:0.6667 
epoch : 14 / 100  :  2022-05-28 15:45:31
train
[14,     0] loss: 0.009, The best rmse/mae: 0.810643 / 0.627984
[14,   100] loss: 0.777, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8492, mae:0.6382 
epoch : 15 / 100  :  2022-05-28 15:47:28
train
[15,     0] loss: 0.008, The best rmse/mae: 0.810643 / 0.627984
[15,   100] loss: 0.745, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8262, mae:0.6270 
epoch : 16 / 100  :  2022-05-28 15:49:12
train
[16,     0] loss: 0.006, The best rmse/mae: 0.810643 / 0.627984
[16,   100] loss: 0.746, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8148, mae:0.6342 
epoch : 17 / 100  :  2022-05-28 15:51:07
train
[17,     0] loss: 0.006, The best rmse/mae: 0.810643 / 0.627984
[17,   100] loss: 0.726, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8223, mae:0.6401 
epoch : 18 / 100  :  2022-05-28 15:53:02
train
[18,     0] loss: 0.007, The best rmse/mae: 0.810643 / 0.627984
[18,   100] loss: 0.699, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8294, mae:0.6245 
epoch : 19 / 100  :  2022-05-28 15:54:49
train
[19,     0] loss: 0.008, The best rmse/mae: 0.810643 / 0.627984
[19,   100] loss: 0.699, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8130, mae:0.6247 
epoch : 20 / 100  :  2022-05-28 15:56:40
train
[20,     0] loss: 0.007, The best rmse/mae: 0.810643 / 0.627984
[20,   100] loss: 0.688, The best rmse/mae: 0.810643 / 0.627984
test
rmse: 0.8027, mae:0.6256 
epoch : 21 / 100  :  2022-05-28 15:58:34
train
[21,     0] loss: 0.008, The best rmse/mae: 0.802697 / 0.625592
[21,   100] loss: 0.672, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8056, mae:0.6300 
epoch : 22 / 100  :  2022-05-28 16:00:27
train
[22,     0] loss: 0.006, The best rmse/mae: 0.802697 / 0.625592
[22,   100] loss: 0.667, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8079, mae:0.6271 
epoch : 23 / 100  :  2022-05-28 16:02:25
train
[23,     0] loss: 0.006, The best rmse/mae: 0.802697 / 0.625592
[23,   100] loss: 0.655, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8643, mae:0.6438 
epoch : 24 / 100  :  2022-05-28 16:04:13
train
[24,     0] loss: 0.005, The best rmse/mae: 0.802697 / 0.625592
[24,   100] loss: 0.647, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8104, mae:0.6234 
epoch : 25 / 100  :  2022-05-28 16:06:04
train
[25,     0] loss: 0.006, The best rmse/mae: 0.802697 / 0.625592
[25,   100] loss: 0.640, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8069, mae:0.6377 
epoch : 26 / 100  :  2022-05-28 16:07:55
train
[26,     0] loss: 0.006, The best rmse/mae: 0.802697 / 0.625592
[26,   100] loss: 0.627, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.8027, mae:0.6254 
epoch : 27 / 100  :  2022-05-28 16:09:49
train
[27,     0] loss: 0.006, The best rmse/mae: 0.802697 / 0.625592
[27,   100] loss: 0.624, The best rmse/mae: 0.802697 / 0.625592
test
rmse: 0.7996, mae:0.6260 
epoch : 28 / 100  :  2022-05-28 16:11:55
train
[28,     0] loss: 0.007, The best rmse/mae: 0.799623 / 0.625963
[28,   100] loss: 0.618, The best rmse/mae: 0.799623 / 0.625963
test
rmse: 0.7996, mae:0.6240 
epoch : 29 / 100  :  2022-05-28 16:13:53
train
[29,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[29,   100] loss: 0.619, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8044, mae:0.6226 
epoch : 30 / 100  :  2022-05-28 16:15:42
train
[30,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[30,   100] loss: 0.608, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8141, mae:0.6210 
epoch : 31 / 100  :  2022-05-28 16:17:44
train
[31,     0] loss: 0.007, The best rmse/mae: 0.799598 / 0.624033
[31,   100] loss: 0.611, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8039, mae:0.6188 
epoch : 32 / 100  :  2022-05-28 16:19:48
train
[32,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[32,   100] loss: 0.599, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8009, mae:0.6258 
epoch : 33 / 100  :  2022-05-28 16:21:54
train
[33,     0] loss: 0.007, The best rmse/mae: 0.799598 / 0.624033
[33,   100] loss: 0.597, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8010, mae:0.6164 
epoch : 34 / 100  :  2022-05-28 16:23:50
train
[34,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[34,   100] loss: 0.595, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8024, mae:0.6212 
epoch : 35 / 100  :  2022-05-28 16:25:42
train
[35,     0] loss: 0.005, The best rmse/mae: 0.799598 / 0.624033
[35,   100] loss: 0.592, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8042, mae:0.6174 
epoch : 36 / 100  :  2022-05-28 16:27:38
train
[36,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[36,   100] loss: 0.598, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8023, mae:0.6334 
epoch : 37 / 100  :  2022-05-28 16:29:31
train
[37,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[37,   100] loss: 0.586, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8324, mae:0.6626 
epoch : 38 / 100  :  2022-05-28 16:31:21
train
[38,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[38,   100] loss: 0.586, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8442, mae:0.6686 
epoch : 39 / 100  :  2022-05-28 16:33:13
train
[39,     0] loss: 0.005, The best rmse/mae: 0.799598 / 0.624033
[39,   100] loss: 0.596, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8209, mae:0.6499 
epoch : 40 / 100  :  2022-05-28 16:35:03
train
[40,     0] loss: 0.005, The best rmse/mae: 0.799598 / 0.624033
[40,   100] loss: 0.590, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8014, mae:0.6242 
epoch : 41 / 100  :  2022-05-28 16:36:55
train
[41,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[41,   100] loss: 0.588, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8047, mae:0.6229 
epoch : 42 / 100  :  2022-05-28 16:38:47
train
[42,     0] loss: 0.007, The best rmse/mae: 0.799598 / 0.624033
[42,   100] loss: 0.596, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8195, mae:0.6573 
epoch : 43 / 100  :  2022-05-28 16:40:43
train
[43,     0] loss: 0.006, The best rmse/mae: 0.799598 / 0.624033
[43,   100] loss: 0.585, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8084, mae:0.6445 
epoch : 44 / 100  :  2022-05-28 16:42:46
train
[44,     0] loss: 0.004, The best rmse/mae: 0.799598 / 0.624033
[44,   100] loss: 0.587, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.8515, mae:0.6883 
epoch : 45 / 100  :  2022-05-28 16:44:42
train
[45,     0] loss: 0.005, The best rmse/mae: 0.799598 / 0.624033
[45,   100] loss: 0.585, The best rmse/mae: 0.799598 / 0.624033
test
rmse: 0.7970, mae:0.6205 
epoch : 46 / 100  :  2022-05-28 16:46:31
train
[46,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[46,   100] loss: 0.591, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8180, mae:0.6473 
epoch : 47 / 100  :  2022-05-28 16:48:18
train
[47,     0] loss: 0.008, The best rmse/mae: 0.796968 / 0.620510
[47,   100] loss: 0.570, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8055, mae:0.6310 
epoch : 48 / 100  :  2022-05-28 16:50:08
train
[48,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[48,   100] loss: 0.580, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8101, mae:0.6343 
epoch : 49 / 100  :  2022-05-28 16:51:55
train
[49,     0] loss: 0.008, The best rmse/mae: 0.796968 / 0.620510
[49,   100] loss: 0.578, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8007, mae:0.6209 
epoch : 50 / 100  :  2022-05-28 16:53:41
train
[50,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[50,   100] loss: 0.584, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8050, mae:0.6195 
epoch : 51 / 100  :  2022-05-28 16:55:31
train
[51,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[51,   100] loss: 0.573, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8027, mae:0.6299 
epoch : 52 / 100  :  2022-05-28 16:57:23
train
[52,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[52,   100] loss: 0.581, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8106, mae:0.6435 
epoch : 53 / 100  :  2022-05-28 16:59:16
train
[53,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[53,   100] loss: 0.576, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8045, mae:0.6343 
epoch : 54 / 100  :  2022-05-28 17:01:16
train
[54,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[54,   100] loss: 0.573, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8000, mae:0.6294 
epoch : 55 / 100  :  2022-05-28 17:03:15
train
[55,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[55,   100] loss: 0.576, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8071, mae:0.6335 
epoch : 56 / 100  :  2022-05-28 17:05:15
train
[56,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[56,   100] loss: 0.573, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8087, mae:0.6440 
epoch : 57 / 100  :  2022-05-28 17:07:12
train
[57,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[57,   100] loss: 0.579, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.7994, mae:0.6266 
epoch : 58 / 100  :  2022-05-28 17:09:04
train
[58,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[58,   100] loss: 0.569, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8123, mae:0.6299 
epoch : 59 / 100  :  2022-05-28 17:10:59
train
[59,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[59,   100] loss: 0.572, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8104, mae:0.6257 
epoch : 60 / 100  :  2022-05-28 17:12:50
train
[60,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[60,   100] loss: 0.576, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8285, mae:0.6548 
epoch : 61 / 100  :  2022-05-28 17:14:44
train
[61,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[61,   100] loss: 0.577, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8173, mae:0.6396 
epoch : 62 / 100  :  2022-05-28 17:16:32
train
[62,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[62,   100] loss: 0.570, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8037, mae:0.6293 
epoch : 63 / 100  :  2022-05-28 17:18:20
train
[63,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[63,   100] loss: 0.574, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8311, mae:0.6734 
epoch : 64 / 100  :  2022-05-28 17:20:07
train
[64,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[64,   100] loss: 0.572, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8174, mae:0.6393 
epoch : 65 / 100  :  2022-05-28 17:22:04
train
[65,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[65,   100] loss: 0.572, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8115, mae:0.6390 
epoch : 66 / 100  :  2022-05-28 17:23:54
train
[66,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[66,   100] loss: 0.569, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8039, mae:0.6240 
epoch : 67 / 100  :  2022-05-28 17:25:43
train
[67,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[67,   100] loss: 0.566, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8111, mae:0.6385 
epoch : 68 / 100  :  2022-05-28 17:27:31
train
[68,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[68,   100] loss: 0.575, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8395, mae:0.6666 
epoch : 69 / 100  :  2022-05-28 17:29:20
train
[69,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[69,   100] loss: 0.572, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8031, mae:0.6375 
epoch : 70 / 100  :  2022-05-28 17:31:20
train
[70,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[70,   100] loss: 0.558, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8109, mae:0.6308 
epoch : 71 / 100  :  2022-05-28 17:33:34
train
[71,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[71,   100] loss: 0.567, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8190, mae:0.6513 
epoch : 72 / 100  :  2022-05-28 17:35:34
train
[72,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[72,   100] loss: 0.565, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8143, mae:0.6265 
epoch : 73 / 100  :  2022-05-28 17:37:29
train
[73,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[73,   100] loss: 0.565, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8059, mae:0.6307 
epoch : 74 / 100  :  2022-05-28 17:39:28
train
[74,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[74,   100] loss: 0.564, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8208, mae:0.6466 
epoch : 75 / 100  :  2022-05-28 17:41:25
train
[75,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[75,   100] loss: 0.568, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8342, mae:0.6774 
epoch : 76 / 100  :  2022-05-28 17:43:18
train
[76,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[76,   100] loss: 0.572, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8098, mae:0.6223 
epoch : 77 / 100  :  2022-05-28 17:45:12
train
[77,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[77,   100] loss: 0.569, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8195, mae:0.6568 
epoch : 78 / 100  :  2022-05-28 17:47:04
train
[78,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[78,   100] loss: 0.565, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8134, mae:0.6470 
epoch : 79 / 100  :  2022-05-28 17:48:57
train
[79,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[79,   100] loss: 0.566, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8037, mae:0.6224 
epoch : 80 / 100  :  2022-05-28 17:50:52
train
[80,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[80,   100] loss: 0.568, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8031, mae:0.6207 
epoch : 81 / 100  :  2022-05-28 17:52:48
train
[81,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[81,   100] loss: 0.564, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8033, mae:0.6344 
epoch : 82 / 100  :  2022-05-28 17:54:44
train
[82,     0] loss: 0.004, The best rmse/mae: 0.796968 / 0.620510
[82,   100] loss: 0.567, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8026, mae:0.6212 
epoch : 83 / 100  :  2022-05-28 17:56:37
train
[83,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[83,   100] loss: 0.558, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8051, mae:0.6260 
epoch : 84 / 100  :  2022-05-28 17:58:30
train
[84,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[84,   100] loss: 0.561, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8195, mae:0.6269 
epoch : 85 / 100  :  2022-05-28 18:00:21
train
[85,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[85,   100] loss: 0.555, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8061, mae:0.6347 
epoch : 86 / 100  :  2022-05-28 18:02:17
train
[86,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[86,   100] loss: 0.559, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8063, mae:0.6264 
epoch : 87 / 100  :  2022-05-28 18:04:06
train
[87,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[87,   100] loss: 0.560, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8067, mae:0.6427 
epoch : 88 / 100  :  2022-05-28 18:06:02
train
[88,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[88,   100] loss: 0.561, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8076, mae:0.6337 
epoch : 89 / 100  :  2022-05-28 18:07:55
train
[89,     0] loss: 0.007, The best rmse/mae: 0.796968 / 0.620510
[89,   100] loss: 0.560, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8070, mae:0.6238 
epoch : 90 / 100  :  2022-05-28 18:09:47
train
[90,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[90,   100] loss: 0.560, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8086, mae:0.6287 
epoch : 91 / 100  :  2022-05-28 18:11:41
train
[91,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[91,   100] loss: 0.560, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8191, mae:0.6397 
epoch : 92 / 100  :  2022-05-28 18:13:42
train
[92,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[92,   100] loss: 0.559, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8262, mae:0.6574 
epoch : 93 / 100  :  2022-05-28 18:15:35
train
[93,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[93,   100] loss: 0.562, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8080, mae:0.6259 
epoch : 94 / 100  :  2022-05-28 18:17:36
train
[94,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[94,   100] loss: 0.559, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8105, mae:0.6307 
epoch : 95 / 100  :  2022-05-28 18:19:29
train
[95,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[95,   100] loss: 0.558, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8101, mae:0.6330 
epoch : 96 / 100  :  2022-05-28 18:21:18
train
[96,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[96,   100] loss: 0.559, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8168, mae:0.6508 
epoch : 97 / 100  :  2022-05-28 18:23:09
train
[97,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[97,   100] loss: 0.565, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8062, mae:0.6321 
epoch : 98 / 100  :  2022-05-28 18:25:02
train
[98,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[98,   100] loss: 0.565, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8284, mae:0.6650 
epoch : 99 / 100  :  2022-05-28 18:26:57
train
[99,     0] loss: 0.005, The best rmse/mae: 0.796968 / 0.620510
[99,   100] loss: 0.566, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8090, mae:0.6238 
epoch : 100 / 100  :  2022-05-28 18:28:53
train
[100,     0] loss: 0.006, The best rmse/mae: 0.796968 / 0.620510
[100,   100] loss: 0.555, The best rmse/mae: 0.796968 / 0.620510
test
rmse: 0.8028, mae:0.6323 




######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################
######################################################v

# DIFFERANCE CONDITION  4
# MODIFY TRUST CONDITION  6
epoch : 1 / 100  :  2022-05-28 21:26:12
train
[1,     0] loss: 0.108, The best rmse/mae: 9999.000000 / 9999.000000
[1,   100] loss: 7.587, The best rmse/mae: 9999.000000 / 9999.000000
test
rmse: 2.4732, mae:2.3095 
epoch : 2 / 100  :  2022-05-28 21:28:04
train
[2,     0] loss: 0.058, The best rmse/mae: 2.473184 / 2.309465
[2,   100] loss: 3.766, The best rmse/mae: 2.473184 / 2.309465
test
rmse: 1.2819, mae:1.1155 
epoch : 3 / 100  :  2022-05-28 21:29:58
train
[3,     0] loss: 0.024, The best rmse/mae: 1.281932 / 1.115550
[3,   100] loss: 2.069, The best rmse/mae: 1.281932 / 1.115550
test
rmse: 1.0270, mae:0.8415 
epoch : 4 / 100  :  2022-05-28 21:31:49
train
[4,     0] loss: 0.016, The best rmse/mae: 1.026958 / 0.841511
[4,   100] loss: 1.529, The best rmse/mae: 1.026958 / 0.841511
test
rmse: 0.8605, mae:0.6685 
epoch : 5 / 100  :  2022-05-28 21:33:46
train
[5,     0] loss: 0.013, The best rmse/mae: 0.860507 / 0.668463
[5,   100] loss: 1.331, The best rmse/mae: 0.860507 / 0.668463
test
rmse: 0.8890, mae:0.7054 
epoch : 6 / 100  :  2022-05-28 21:35:43
train
[6,     0] loss: 0.013, The best rmse/mae: 0.860507 / 0.668463
[6,   100] loss: 1.203, The best rmse/mae: 0.860507 / 0.668463
test
rmse: 0.9653, mae:0.7108 
epoch : 7 / 100  :  2022-05-28 21:37:39
train
[7,     0] loss: 0.010, The best rmse/mae: 0.860507 / 0.668463
[7,   100] loss: 1.120, The best rmse/mae: 0.860507 / 0.668463
test
rmse: 0.8745, mae:0.6546 
epoch : 8 / 100  :  2022-05-28 21:39:31
train
[8,     0] loss: 0.011, The best rmse/mae: 0.860507 / 0.668463
[8,   100] loss: 1.038, The best rmse/mae: 0.860507 / 0.668463
test
rmse: 0.8200, mae:0.6427 
epoch : 9 / 100  :  2022-05-28 21:41:17
train
[9,     0] loss: 0.011, The best rmse/mae: 0.819968 / 0.642742
[9,   100] loss: 0.980, The best rmse/mae: 0.819968 / 0.642742
test
rmse: 0.8124, mae:0.6405 
epoch : 10 / 100  :  2022-05-28 21:43:05
train
[10,     0] loss: 0.011, The best rmse/mae: 0.812389 / 0.640467
[10,   100] loss: 0.938, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8251, mae:0.6448 
epoch : 11 / 100  :  2022-05-28 21:44:51
train
[11,     0] loss: 0.007, The best rmse/mae: 0.812389 / 0.640467
[11,   100] loss: 0.901, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8247, mae:0.6270 
epoch : 12 / 100  :  2022-05-28 21:46:37
train
[12,     0] loss: 0.009, The best rmse/mae: 0.812389 / 0.640467
[12,   100] loss: 0.875, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8423, mae:0.6554 
epoch : 13 / 100  :  2022-05-28 21:48:27
train
[13,     0] loss: 0.009, The best rmse/mae: 0.812389 / 0.640467
[13,   100] loss: 0.840, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8148, mae:0.6385 
epoch : 14 / 100  :  2022-05-28 21:50:13
train
[14,     0] loss: 0.008, The best rmse/mae: 0.812389 / 0.640467
[14,   100] loss: 0.811, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8694, mae:0.6500 
epoch : 15 / 100  :  2022-05-28 21:52:05
train
[15,     0] loss: 0.007, The best rmse/mae: 0.812389 / 0.640467
[15,   100] loss: 0.786, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8960, mae:0.6618 
epoch : 16 / 100  :  2022-05-28 21:53:54
train
[16,     0] loss: 0.008, The best rmse/mae: 0.812389 / 0.640467
[16,   100] loss: 0.774, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8277, mae:0.6308 
epoch : 17 / 100  :  2022-05-28 21:55:39
train
[17,     0] loss: 0.007, The best rmse/mae: 0.812389 / 0.640467
[17,   100] loss: 0.747, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8134, mae:0.6380 
epoch : 18 / 100  :  2022-05-28 21:57:25
train
[18,     0] loss: 0.006, The best rmse/mae: 0.812389 / 0.640467
[18,   100] loss: 0.726, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8339, mae:0.6284 
epoch : 19 / 100  :  2022-05-28 21:59:10
train
[19,     0] loss: 0.007, The best rmse/mae: 0.812389 / 0.640467
[19,   100] loss: 0.714, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8346, mae:0.6305 
epoch : 20 / 100  :  2022-05-28 22:00:56
train
[20,     0] loss: 0.006, The best rmse/mae: 0.812389 / 0.640467
[20,   100] loss: 0.708, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8344, mae:0.6493 
epoch : 21 / 100  :  2022-05-28 22:02:54
train
[21,     0] loss: 0.006, The best rmse/mae: 0.812389 / 0.640467
[21,   100] loss: 0.698, The best rmse/mae: 0.812389 / 0.640467
test
rmse: 0.8060, mae:0.6210 
epoch : 22 / 100  :  2022-05-28 22:04:45
train
[22,     0] loss: 0.005, The best rmse/mae: 0.806032 / 0.620965
[22,   100] loss: 0.672, The best rmse/mae: 0.806032 / 0.620965
test
rmse: 0.8203, mae:0.6437 
epoch : 23 / 100  :  2022-05-28 22:06:30
train
[23,     0] loss: 0.007, The best rmse/mae: 0.806032 / 0.620965
[23,   100] loss: 0.649, The best rmse/mae: 0.806032 / 0.620965
test
rmse: 0.8025, mae:0.6287 
epoch : 24 / 100  :  2022-05-28 22:08:18
train
[24,     0] loss: 0.007, The best rmse/mae: 0.802476 / 0.628661
[24,   100] loss: 0.657, The best rmse/mae: 0.802476 / 0.628661
test
rmse: 0.8190, mae:0.6226 
epoch : 25 / 100  :  2022-05-28 22:10:05
train
[25,     0] loss: 0.006, The best rmse/mae: 0.802476 / 0.628661
[25,   100] loss: 0.642, The best rmse/mae: 0.802476 / 0.628661
test
rmse: 0.8024, mae:0.6302 
epoch : 26 / 100  :  2022-05-28 22:12:01
train
[26,     0] loss: 0.007, The best rmse/mae: 0.802408 / 0.630170
[26,   100] loss: 0.637, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8455, mae:0.6302 
epoch : 27 / 100  :  2022-05-28 22:13:54
train
[27,     0] loss: 0.007, The best rmse/mae: 0.802408 / 0.630170
[27,   100] loss: 0.625, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8111, mae:0.6268 
epoch : 28 / 100  :  2022-05-28 22:15:43
train
[28,     0] loss: 0.006, The best rmse/mae: 0.802408 / 0.630170
[28,   100] loss: 0.625, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8064, mae:0.6420 
epoch : 29 / 100  :  2022-05-28 22:17:29
train
[29,     0] loss: 0.006, The best rmse/mae: 0.802408 / 0.630170
[29,   100] loss: 0.613, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8105, mae:0.6440 
epoch : 30 / 100  :  2022-05-28 22:19:15
train
[30,     0] loss: 0.006, The best rmse/mae: 0.802408 / 0.630170
[30,   100] loss: 0.617, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8335, mae:0.6607 
epoch : 31 / 100  :  2022-05-28 22:21:04
train
[31,     0] loss: 0.005, The best rmse/mae: 0.802408 / 0.630170
[31,   100] loss: 0.615, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8067, mae:0.6242 
epoch : 32 / 100  :  2022-05-28 22:22:50
train
[32,     0] loss: 0.006, The best rmse/mae: 0.802408 / 0.630170
[32,   100] loss: 0.613, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8041, mae:0.6259 
epoch : 33 / 100  :  2022-05-28 22:24:36
train
[33,     0] loss: 0.007, The best rmse/mae: 0.802408 / 0.630170
[33,   100] loss: 0.609, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8080, mae:0.6284 
epoch : 34 / 100  :  2022-05-28 22:26:23
train
[34,     0] loss: 0.005, The best rmse/mae: 0.802408 / 0.630170
[34,   100] loss: 0.599, The best rmse/mae: 0.802408 / 0.630170
test
rmse: 0.8023, mae:0.6341 
epoch : 35 / 100  :  2022-05-28 22:28:15
train
[35,     0] loss: 0.006, The best rmse/mae: 0.802261 / 0.634132
[35,   100] loss: 0.604, The best rmse/mae: 0.802261 / 0.634132
test
rmse: 0.8020, mae:0.6285 
epoch : 36 / 100  :  2022-05-28 22:30:04
train
[36,     0] loss: 0.007, The best rmse/mae: 0.801970 / 0.628525
[36,   100] loss: 0.599, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8036, mae:0.6365 
epoch : 37 / 100  :  2022-05-28 22:31:51
train
[37,     0] loss: 0.007, The best rmse/mae: 0.801970 / 0.628525
[37,   100] loss: 0.593, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8237, mae:0.6390 
epoch : 38 / 100  :  2022-05-28 22:33:38
train
[38,     0] loss: 0.007, The best rmse/mae: 0.801970 / 0.628525
[38,   100] loss: 0.589, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8045, mae:0.6355 
epoch : 39 / 100  :  2022-05-28 22:35:23
train
[39,     0] loss: 0.006, The best rmse/mae: 0.801970 / 0.628525
[39,   100] loss: 0.590, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8037, mae:0.6374 
epoch : 40 / 100  :  2022-05-28 22:37:09
train
[40,     0] loss: 0.006, The best rmse/mae: 0.801970 / 0.628525
[40,   100] loss: 0.597, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8161, mae:0.6489 
epoch : 41 / 100  :  2022-05-28 22:38:55
train
[41,     0] loss: 0.005, The best rmse/mae: 0.801970 / 0.628525
[41,   100] loss: 0.592, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8034, mae:0.6284 
epoch : 42 / 100  :  2022-05-28 22:40:40
train
[42,     0] loss: 0.004, The best rmse/mae: 0.801970 / 0.628525
[42,   100] loss: 0.598, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8107, mae:0.6424 
epoch : 43 / 100  :  2022-05-28 22:42:25
train
[43,     0] loss: 0.005, The best rmse/mae: 0.801970 / 0.628525
[43,   100] loss: 0.593, The best rmse/mae: 0.801970 / 0.628525
test
rmse: 0.8016, mae:0.6252 
epoch : 44 / 100  :  2022-05-28 22:44:14
train
[44,     0] loss: 0.006, The best rmse/mae: 0.801630 / 0.625187
[44,   100] loss: 0.586, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8141, mae:0.6421 
epoch : 45 / 100  :  2022-05-28 22:46:03
train
[45,     0] loss: 0.005, The best rmse/mae: 0.801630 / 0.625187
[45,   100] loss: 0.586, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8335, mae:0.6737 
epoch : 46 / 100  :  2022-05-28 22:47:53
train
[46,     0] loss: 0.007, The best rmse/mae: 0.801630 / 0.625187
[46,   100] loss: 0.586, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8267, mae:0.6688 
epoch : 47 / 100  :  2022-05-28 22:49:52
train
[47,     0] loss: 0.006, The best rmse/mae: 0.801630 / 0.625187
[47,   100] loss: 0.588, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8109, mae:0.6275 
epoch : 48 / 100  :  2022-05-28 22:51:40
train
[48,     0] loss: 0.006, The best rmse/mae: 0.801630 / 0.625187
[48,   100] loss: 0.580, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8064, mae:0.6402 
epoch : 49 / 100  :  2022-05-28 22:53:29
train
[49,     0] loss: 0.007, The best rmse/mae: 0.801630 / 0.625187
[49,   100] loss: 0.587, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8214, mae:0.6592 
epoch : 50 / 100  :  2022-05-28 22:55:24
train
[50,     0] loss: 0.006, The best rmse/mae: 0.801630 / 0.625187
[50,   100] loss: 0.578, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8040, mae:0.6346 
epoch : 51 / 100  :  2022-05-28 22:57:20
train
[51,     0] loss: 0.007, The best rmse/mae: 0.801630 / 0.625187
[51,   100] loss: 0.587, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8070, mae:0.6270 
epoch : 52 / 100  :  2022-05-28 22:59:09
train
[52,     0] loss: 0.007, The best rmse/mae: 0.801630 / 0.625187
[52,   100] loss: 0.585, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.8085, mae:0.6469 
epoch : 53 / 100  :  2022-05-28 23:00:57
train
[53,     0] loss: 0.007, The best rmse/mae: 0.801630 / 0.625187
[53,   100] loss: 0.579, The best rmse/mae: 0.801630 / 0.625187
test
rmse: 0.7991, mae:0.6314 
epoch : 54 / 100  :  2022-05-28 23:02:47
train
[54,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[54,   100] loss: 0.585, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8098, mae:0.6305 
epoch : 55 / 100  :  2022-05-28 23:04:36
train
[55,     0] loss: 0.004, The best rmse/mae: 0.799136 / 0.631381
[55,   100] loss: 0.583, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8016, mae:0.6329 
epoch : 56 / 100  :  2022-05-28 23:06:26
train
[56,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[56,   100] loss: 0.576, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8332, mae:0.6720 
epoch : 57 / 100  :  2022-05-28 23:08:19
train
[57,     0] loss: 0.004, The best rmse/mae: 0.799136 / 0.631381
[57,   100] loss: 0.582, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8012, mae:0.6299 
epoch : 58 / 100  :  2022-05-28 23:10:07
train
[58,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[58,   100] loss: 0.585, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8101, mae:0.6424 
epoch : 59 / 100  :  2022-05-28 23:11:56
train
[59,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[59,   100] loss: 0.575, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8025, mae:0.6271 
epoch : 60 / 100  :  2022-05-28 23:13:50
train
[60,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[60,   100] loss: 0.575, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8047, mae:0.6399 
epoch : 61 / 100  :  2022-05-28 23:15:38
train
[61,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[61,   100] loss: 0.576, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8080, mae:0.6347 
epoch : 62 / 100  :  2022-05-28 23:17:26
train
[62,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[62,   100] loss: 0.574, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8090, mae:0.6410 
epoch : 63 / 100  :  2022-05-28 23:19:16
train
[63,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[63,   100] loss: 0.572, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8137, mae:0.6340 
epoch : 64 / 100  :  2022-05-28 23:21:06
train
[64,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[64,   100] loss: 0.580, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8076, mae:0.6357 
epoch : 65 / 100  :  2022-05-28 23:22:58
train
[65,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[65,   100] loss: 0.565, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8119, mae:0.6374 
epoch : 66 / 100  :  2022-05-28 23:24:46
train
[66,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[66,   100] loss: 0.565, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8100, mae:0.6249 
epoch : 67 / 100  :  2022-05-28 23:26:35
train
[67,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[67,   100] loss: 0.580, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8332, mae:0.6517 
epoch : 68 / 100  :  2022-05-28 23:28:24
train
[68,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[68,   100] loss: 0.570, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8120, mae:0.6429 
epoch : 69 / 100  :  2022-05-28 23:30:12
train
[69,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[69,   100] loss: 0.572, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8162, mae:0.6527 
epoch : 70 / 100  :  2022-05-28 23:31:57
train
[70,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[70,   100] loss: 0.572, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8053, mae:0.6297 
epoch : 71 / 100  :  2022-05-28 23:33:46
train
[71,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[71,   100] loss: 0.572, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8089, mae:0.6284 
epoch : 72 / 100  :  2022-05-28 23:35:32
train
[72,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[72,   100] loss: 0.569, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8039, mae:0.6260 
epoch : 73 / 100  :  2022-05-28 23:37:19
train
[73,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[73,   100] loss: 0.569, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8168, mae:0.6411 
epoch : 74 / 100  :  2022-05-28 23:39:07
train
[74,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[74,   100] loss: 0.563, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8074, mae:0.6344 
epoch : 75 / 100  :  2022-05-28 23:40:56
train
[75,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[75,   100] loss: 0.568, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8125, mae:0.6396 
epoch : 76 / 100  :  2022-05-28 23:42:42
train
[76,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[76,   100] loss: 0.565, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8261, mae:0.6633 
epoch : 77 / 100  :  2022-05-28 23:44:35
train
[77,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[77,   100] loss: 0.567, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8640, mae:0.7009 
epoch : 78 / 100  :  2022-05-28 23:46:31
train
[78,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[78,   100] loss: 0.563, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8178, mae:0.6455 
epoch : 79 / 100  :  2022-05-28 23:48:18
train
[79,     0] loss: 0.004, The best rmse/mae: 0.799136 / 0.631381
[79,   100] loss: 0.565, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8579, mae:0.6958 
epoch : 80 / 100  :  2022-05-28 23:50:03
train
[80,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[80,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8149, mae:0.6384 
epoch : 81 / 100  :  2022-05-28 23:51:50
train
[81,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[81,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8414, mae:0.6334 
epoch : 82 / 100  :  2022-05-28 23:53:53
train
[82,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[82,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8002, mae:0.6268 
epoch : 83 / 100  :  2022-05-28 23:55:47
train
[83,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[83,   100] loss: 0.568, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8122, mae:0.6396 
epoch : 84 / 100  :  2022-05-28 23:57:43
train
[84,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[84,   100] loss: 0.563, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8101, mae:0.6354 
epoch : 85 / 100  :  2022-05-28 23:59:57
train
[85,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[85,   100] loss: 0.572, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8342, mae:0.6711 
epoch : 86 / 100  :  2022-05-29 00:01:49
train
[86,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[86,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8201, mae:0.6266 
epoch : 87 / 100  :  2022-05-29 00:03:49
train
[87,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[87,   100] loss: 0.563, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8073, mae:0.6259 
epoch : 88 / 100  :  2022-05-29 00:05:47
train
[88,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[88,   100] loss: 0.562, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8083, mae:0.6296 
epoch : 89 / 100  :  2022-05-29 00:07:44
train
[89,     0] loss: 0.004, The best rmse/mae: 0.799136 / 0.631381
[89,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8045, mae:0.6271 
epoch : 90 / 100  :  2022-05-29 00:09:43
train
[90,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[90,   100] loss: 0.567, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8153, mae:0.6545 
epoch : 91 / 100  :  2022-05-29 00:11:36
train
[91,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[91,   100] loss: 0.564, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8145, mae:0.6458 
epoch : 92 / 100  :  2022-05-29 00:13:29
train
[92,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[92,   100] loss: 0.569, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8159, mae:0.6449 
epoch : 93 / 100  :  2022-05-29 00:15:20
train
[93,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[93,   100] loss: 0.557, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8159, mae:0.6331 
epoch : 94 / 100  :  2022-05-29 00:17:12
train
[94,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[94,   100] loss: 0.564, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8115, mae:0.6429 
epoch : 95 / 100  :  2022-05-29 00:19:06
train
[95,     0] loss: 0.006, The best rmse/mae: 0.799136 / 0.631381
[95,   100] loss: 0.565, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8160, mae:0.6250 
epoch : 96 / 100  :  2022-05-29 00:20:55
train
[96,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[96,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8033, mae:0.6232 
epoch : 97 / 100  :  2022-05-29 00:22:40
train
[97,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[97,   100] loss: 0.564, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8232, mae:0.6326 
epoch : 98 / 100  :  2022-05-29 00:24:22
train
[98,     0] loss: 0.004, The best rmse/mae: 0.799136 / 0.631381
[98,   100] loss: 0.570, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8182, mae:0.6475 
epoch : 99 / 100  :  2022-05-29 00:26:06
train
[99,     0] loss: 0.005, The best rmse/mae: 0.799136 / 0.631381
[99,   100] loss: 0.554, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8119, mae:0.6233 
epoch : 100 / 100  :  2022-05-29 00:27:50
train
[100,     0] loss: 0.007, The best rmse/mae: 0.799136 / 0.631381
[100,   100] loss: 0.566, The best rmse/mae: 0.799136 / 0.631381
test
rmse: 0.8064, mae:0.6265




#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

#######################################################

# DIFFERANCE CONDITION  4
# MODIFY TRUST CONDITION  12
epoch : 1 / 100  :  2022-05-29 01:46:32
train
[1,     0] loss: 0.109, The best rmse/mae: 9999.000000 / 9999.000000
[1,   100] loss: 8.174, The best rmse/mae: 9999.000000 / 9999.000000
test
rmse: 2.4812, mae:2.3188 
epoch : 2 / 100  :  2022-05-29 01:48:18
train
[2,     0] loss: 0.054, The best rmse/mae: 2.481172 / 2.318807
[2,   100] loss: 4.295, The best rmse/mae: 2.481172 / 2.318807
test
rmse: 1.4530, mae:1.2892 
epoch : 3 / 100  :  2022-05-29 01:50:02
train
[3,     0] loss: 0.026, The best rmse/mae: 1.453011 / 1.289179
[3,   100] loss: 1.970, The best rmse/mae: 1.453011 / 1.289179
test
rmse: 0.9842, mae:0.7771 
epoch : 4 / 100  :  2022-05-29 01:51:50
train
[4,     0] loss: 0.017, The best rmse/mae: 0.984198 / 0.777078
[4,   100] loss: 1.444, The best rmse/mae: 0.984198 / 0.777078
test
rmse: 1.1172, mae:0.8669 
epoch : 5 / 100  :  2022-05-29 01:53:34
train
[5,     0] loss: 0.013, The best rmse/mae: 0.984198 / 0.777078
[5,   100] loss: 1.220, The best rmse/mae: 0.984198 / 0.777078
test
rmse: 0.8608, mae:0.6536 
epoch : 6 / 100  :  2022-05-29 01:55:20
train
[6,     0] loss: 0.017, The best rmse/mae: 0.860782 / 0.653618
[6,   100] loss: 1.099, The best rmse/mae: 0.860782 / 0.653618
test
rmse: 0.8870, mae:0.6620 
epoch : 7 / 100  :  2022-05-29 01:57:07
train
[7,     0] loss: 0.010, The best rmse/mae: 0.860782 / 0.653618
[7,   100] loss: 1.017, The best rmse/mae: 0.860782 / 0.653618
test
rmse: 0.8326, mae:0.6383 
epoch : 8 / 100  :  2022-05-29 01:58:52
train
[8,     0] loss: 0.010, The best rmse/mae: 0.832619 / 0.638274
[8,   100] loss: 0.958, The best rmse/mae: 0.832619 / 0.638274
test
rmse: 0.8302, mae:0.6544 
epoch : 9 / 100  :  2022-05-29 02:00:39
train
[9,     0] loss: 0.009, The best rmse/mae: 0.830165 / 0.654416
[9,   100] loss: 0.907, The best rmse/mae: 0.830165 / 0.654416
test
rmse: 0.8751, mae:0.6470 
epoch : 10 / 100  :  2022-05-29 02:02:25
train
[10,     0] loss: 0.007, The best rmse/mae: 0.830165 / 0.654416
[10,   100] loss: 0.882, The best rmse/mae: 0.830165 / 0.654416
test
rmse: 0.8557, mae:0.6408 
epoch : 11 / 100  :  2022-05-29 02:04:11
train
[11,     0] loss: 0.010, The best rmse/mae: 0.830165 / 0.654416
[11,   100] loss: 0.836, The best rmse/mae: 0.830165 / 0.654416
test
rmse: 0.8639, mae:0.6495 
epoch : 12 / 100  :  2022-05-29 02:05:58
train
[12,     0] loss: 0.009, The best rmse/mae: 0.830165 / 0.654416
[12,   100] loss: 0.811, The best rmse/mae: 0.830165 / 0.654416
test
rmse: 0.8654, mae:0.6416 
epoch : 13 / 100  :  2022-05-29 02:07:43
train
[13,     0] loss: 0.009, The best rmse/mae: 0.830165 / 0.654416
[13,   100] loss: 0.781, The best rmse/mae: 0.830165 / 0.654416
test
rmse: 0.8120, mae:0.6247 
epoch : 14 / 100  :  2022-05-29 02:09:31
train
[14,     0] loss: 0.007, The best rmse/mae: 0.812049 / 0.624717
[14,   100] loss: 0.765, The best rmse/mae: 0.812049 / 0.624717
test
rmse: 0.8261, mae:0.6266 
epoch : 15 / 100  :  2022-05-29 02:11:20
train
[15,     0] loss: 0.009, The best rmse/mae: 0.812049 / 0.624717
[15,   100] loss: 0.748, The best rmse/mae: 0.812049 / 0.624717
test
rmse: 0.8044, mae:0.6236 
epoch : 16 / 100  :  2022-05-29 02:13:08
train
[16,     0] loss: 0.008, The best rmse/mae: 0.804442 / 0.623602
[16,   100] loss: 0.730, The best rmse/mae: 0.804442 / 0.623602
test
rmse: 0.8535, mae:0.6376 
epoch : 17 / 100  :  2022-05-29 02:14:57
train
[17,     0] loss: 0.007, The best rmse/mae: 0.804442 / 0.623602
[17,   100] loss: 0.710, The best rmse/mae: 0.804442 / 0.623602
test
rmse: 0.8210, mae:0.6456 
epoch : 18 / 100  :  2022-05-29 02:16:45
train
[18,     0] loss: 0.007, The best rmse/mae: 0.804442 / 0.623602
[18,   100] loss: 0.700, The best rmse/mae: 0.804442 / 0.623602
test
rmse: 0.8042, mae:0.6216 
epoch : 19 / 100  :  2022-05-29 02:18:34
train
[19,     0] loss: 0.006, The best rmse/mae: 0.804216 / 0.621644
[19,   100] loss: 0.679, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8058, mae:0.6258 
epoch : 20 / 100  :  2022-05-29 02:20:23
train
[20,     0] loss: 0.007, The best rmse/mae: 0.804216 / 0.621644
[20,   100] loss: 0.676, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8378, mae:0.6306 
epoch : 21 / 100  :  2022-05-29 02:22:09
train
[21,     0] loss: 0.006, The best rmse/mae: 0.804216 / 0.621644
[21,   100] loss: 0.667, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8127, mae:0.6214 
epoch : 22 / 100  :  2022-05-29 02:23:55
train
[22,     0] loss: 0.006, The best rmse/mae: 0.804216 / 0.621644
[22,   100] loss: 0.651, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8054, mae:0.6311 
epoch : 23 / 100  :  2022-05-29 02:25:46
train
[23,     0] loss: 0.006, The best rmse/mae: 0.804216 / 0.621644
[23,   100] loss: 0.635, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8289, mae:0.6246 
epoch : 24 / 100  :  2022-05-29 02:27:33
train
[24,     0] loss: 0.007, The best rmse/mae: 0.804216 / 0.621644
[24,   100] loss: 0.632, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8140, mae:0.6305 
epoch : 25 / 100  :  2022-05-29 02:29:22
train
[25,     0] loss: 0.006, The best rmse/mae: 0.804216 / 0.621644
[25,   100] loss: 0.629, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8043, mae:0.6382 
epoch : 26 / 100  :  2022-05-29 02:31:12
train
[26,     0] loss: 0.007, The best rmse/mae: 0.804216 / 0.621644
[26,   100] loss: 0.624, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.8329, mae:0.6489 
epoch : 27 / 100  :  2022-05-29 02:33:00
train
[27,     0] loss: 0.007, The best rmse/mae: 0.804216 / 0.621644
[27,   100] loss: 0.613, The best rmse/mae: 0.804216 / 0.621644
test
rmse: 0.7993, mae:0.6163 
epoch : 28 / 100  :  2022-05-29 02:34:46
train
[28,     0] loss: 0.006, The best rmse/mae: 0.799317 / 0.616305
[28,   100] loss: 0.618, The best rmse/mae: 0.799317 / 0.616305
test
rmse: 0.7942, mae:0.6160 
epoch : 29 / 100  :  2022-05-29 02:36:31
train
[29,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[29,   100] loss: 0.610, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8575, mae:0.6824 
epoch : 30 / 100  :  2022-05-29 02:38:16
train
[30,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[30,   100] loss: 0.608, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8079, mae:0.6234 
epoch : 31 / 100  :  2022-05-29 02:40:01
train
[31,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[31,   100] loss: 0.602, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8145, mae:0.6202 
epoch : 32 / 100  :  2022-05-29 02:41:46
train
[32,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[32,   100] loss: 0.609, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8033, mae:0.6388 
epoch : 33 / 100  :  2022-05-29 02:43:30
train
[33,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[33,   100] loss: 0.594, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8026, mae:0.6215 
epoch : 34 / 100  :  2022-05-29 02:45:15
train
[34,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[34,   100] loss: 0.590, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8217, mae:0.6497 
epoch : 35 / 100  :  2022-05-29 02:46:59
train
[35,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[35,   100] loss: 0.592, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8038, mae:0.6256 
epoch : 36 / 100  :  2022-05-29 02:48:43
train
[36,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[36,   100] loss: 0.592, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8086, mae:0.6359 
epoch : 37 / 100  :  2022-05-29 02:50:28
train
[37,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[37,   100] loss: 0.592, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8033, mae:0.6273 
epoch : 38 / 100  :  2022-05-29 02:52:13
train
[38,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[38,   100] loss: 0.584, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8143, mae:0.6413 
epoch : 39 / 100  :  2022-05-29 02:53:57
train
[39,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[39,   100] loss: 0.590, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8045, mae:0.6300 
epoch : 40 / 100  :  2022-05-29 02:55:40
train
[40,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[40,   100] loss: 0.590, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8131, mae:0.6220 
epoch : 41 / 100  :  2022-05-29 02:57:25
train
[41,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[41,   100] loss: 0.593, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8002, mae:0.6267 
epoch : 42 / 100  :  2022-05-29 02:59:09
train
[42,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[42,   100] loss: 0.585, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8043, mae:0.6196 
epoch : 43 / 100  :  2022-05-29 03:00:54
train
[43,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[43,   100] loss: 0.589, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8088, mae:0.6242 
epoch : 44 / 100  :  2022-05-29 03:02:38
train
[44,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[44,   100] loss: 0.584, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8039, mae:0.6255 
epoch : 45 / 100  :  2022-05-29 03:04:23
train
[45,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[45,   100] loss: 0.580, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8055, mae:0.6223 
epoch : 46 / 100  :  2022-05-29 03:06:08
train
[46,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[46,   100] loss: 0.585, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8137, mae:0.6200 
epoch : 47 / 100  :  2022-05-29 03:07:53
train
[47,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[47,   100] loss: 0.579, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.7995, mae:0.6183 
epoch : 48 / 100  :  2022-05-29 03:09:37
train
[48,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[48,   100] loss: 0.578, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8019, mae:0.6218 
epoch : 49 / 100  :  2022-05-29 03:11:23
train
[49,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[49,   100] loss: 0.578, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8080, mae:0.6354 
epoch : 50 / 100  :  2022-05-29 03:13:07
train
[50,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[50,   100] loss: 0.574, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8000, mae:0.6236 
epoch : 51 / 100  :  2022-05-29 03:14:52
train
[51,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[51,   100] loss: 0.579, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8059, mae:0.6277 
epoch : 52 / 100  :  2022-05-29 03:16:36
train
[52,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[52,   100] loss: 0.575, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8292, mae:0.6298 
epoch : 53 / 100  :  2022-05-29 03:18:21
train
[53,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[53,   100] loss: 0.572, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8457, mae:0.6830 
epoch : 54 / 100  :  2022-05-29 03:20:06
train
[54,     0] loss: 0.008, The best rmse/mae: 0.794178 / 0.615951
[54,   100] loss: 0.579, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8017, mae:0.6224 
epoch : 55 / 100  :  2022-05-29 03:21:51
train
[55,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[55,   100] loss: 0.580, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8171, mae:0.6404 
epoch : 56 / 100  :  2022-05-29 03:23:38
train
[56,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[56,   100] loss: 0.567, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8188, mae:0.6236 
epoch : 57 / 100  :  2022-05-29 03:25:23
train
[57,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[57,   100] loss: 0.569, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8123, mae:0.6297 
epoch : 58 / 100  :  2022-05-29 03:27:07
train
[58,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[58,   100] loss: 0.569, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8223, mae:0.6526 
epoch : 59 / 100  :  2022-05-29 03:28:51
train
[59,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[59,   100] loss: 0.574, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8157, mae:0.6378 
epoch : 60 / 100  :  2022-05-29 03:30:35
train
[60,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[60,   100] loss: 0.563, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8104, mae:0.6402 
epoch : 61 / 100  :  2022-05-29 03:32:19
train
[61,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[61,   100] loss: 0.571, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8345, mae:0.6691 
epoch : 62 / 100  :  2022-05-29 03:34:03
train
[62,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[62,   100] loss: 0.571, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8043, mae:0.6259 
epoch : 63 / 100  :  2022-05-29 03:35:47
train
[63,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[63,   100] loss: 0.576, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8133, mae:0.6265 
epoch : 64 / 100  :  2022-05-29 03:37:30
train
[64,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[64,   100] loss: 0.574, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8063, mae:0.6274 
epoch : 65 / 100  :  2022-05-29 03:39:14
train
[65,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[65,   100] loss: 0.571, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8258, mae:0.6680 
epoch : 66 / 100  :  2022-05-29 03:40:57
train
[66,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[66,   100] loss: 0.566, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8198, mae:0.6408 
epoch : 67 / 100  :  2022-05-29 03:42:43
train
[67,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[67,   100] loss: 0.564, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8060, mae:0.6249 
epoch : 68 / 100  :  2022-05-29 03:44:30
train
[68,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[68,   100] loss: 0.564, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8053, mae:0.6355 
epoch : 69 / 100  :  2022-05-29 03:46:19
train
[69,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[69,   100] loss: 0.573, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8424, mae:0.6595 
epoch : 70 / 100  :  2022-05-29 03:48:14
train
[70,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[70,   100] loss: 0.564, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8350, mae:0.6721 
epoch : 71 / 100  :  2022-05-29 03:50:03
train
[71,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[71,   100] loss: 0.569, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8137, mae:0.6427 
epoch : 72 / 100  :  2022-05-29 03:51:49
train
[72,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[72,   100] loss: 0.568, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8039, mae:0.6331 
epoch : 73 / 100  :  2022-05-29 03:53:34
train
[73,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[73,   100] loss: 0.560, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8336, mae:0.6683 
epoch : 74 / 100  :  2022-05-29 03:55:19
train
[74,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[74,   100] loss: 0.569, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8052, mae:0.6437 
epoch : 75 / 100  :  2022-05-29 03:57:04
train
[75,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[75,   100] loss: 0.563, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8032, mae:0.6292 
epoch : 76 / 100  :  2022-05-29 03:58:55
train
[76,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[76,   100] loss: 0.571, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8056, mae:0.6238 
epoch : 77 / 100  :  2022-05-29 04:00:45
train
[77,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[77,   100] loss: 0.559, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8083, mae:0.6381 
epoch : 78 / 100  :  2022-05-29 04:02:36
train
[78,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[78,   100] loss: 0.566, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8250, mae:0.6617 
epoch : 79 / 100  :  2022-05-29 04:04:27
train
[79,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[79,   100] loss: 0.564, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8222, mae:0.6613 
epoch : 80 / 100  :  2022-05-29 04:06:21
train
[80,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[80,   100] loss: 0.566, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8153, mae:0.6299 
epoch : 81 / 100  :  2022-05-29 04:08:19
train
[81,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[81,   100] loss: 0.567, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8129, mae:0.6293 
epoch : 82 / 100  :  2022-05-29 04:10:08
train
[82,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[82,   100] loss: 0.560, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8131, mae:0.6360 
epoch : 83 / 100  :  2022-05-29 04:12:00
train
[83,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[83,   100] loss: 0.561, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8180, mae:0.6421 
epoch : 84 / 100  :  2022-05-29 04:13:50
train
[84,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[84,   100] loss: 0.565, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8118, mae:0.6392 
epoch : 85 / 100  :  2022-05-29 04:15:43
train
[85,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[85,   100] loss: 0.562, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8181, mae:0.6491 
epoch : 86 / 100  :  2022-05-29 04:17:39
train
[86,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[86,   100] loss: 0.570, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8092, mae:0.6424 
epoch : 87 / 100  :  2022-05-29 04:19:33
train
[87,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[87,   100] loss: 0.570, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8149, mae:0.6355 
epoch : 88 / 100  :  2022-05-29 04:21:26
train
[88,     0] loss: 0.007, The best rmse/mae: 0.794178 / 0.615951
[88,   100] loss: 0.560, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.7986, mae:0.6241 
epoch : 89 / 100  :  2022-05-29 04:23:17
train
[89,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[89,   100] loss: 0.566, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8198, mae:0.6479 
epoch : 90 / 100  :  2022-05-29 04:25:13
train
[90,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[90,   100] loss: 0.560, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8130, mae:0.6440 
epoch : 91 / 100  :  2022-05-29 04:27:12
train
[91,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[91,   100] loss: 0.564, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8159, mae:0.6407 
epoch : 92 / 100  :  2022-05-29 04:29:05
train
[92,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[92,   100] loss: 0.558, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8477, mae:0.6844 
epoch : 93 / 100  :  2022-05-29 04:30:52
train
[93,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[93,   100] loss: 0.558, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8022, mae:0.6239 
epoch : 94 / 100  :  2022-05-29 04:32:43
train
[94,     0] loss: 0.004, The best rmse/mae: 0.794178 / 0.615951
[94,   100] loss: 0.565, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8231, mae:0.6474 
epoch : 95 / 100  :  2022-05-29 04:34:34
train
[95,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[95,   100] loss: 0.565, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8095, mae:0.6337 
epoch : 96 / 100  :  2022-05-29 04:36:28
train
[96,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[96,   100] loss: 0.559, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8177, mae:0.6542 
epoch : 97 / 100  :  2022-05-29 04:38:31
train
[97,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[97,   100] loss: 0.566, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8123, mae:0.6415 
epoch : 98 / 100  :  2022-05-29 04:40:30
train
[98,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[98,   100] loss: 0.559, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8115, mae:0.6387 
epoch : 99 / 100  :  2022-05-29 04:42:22
train
[99,     0] loss: 0.005, The best rmse/mae: 0.794178 / 0.615951
[99,   100] loss: 0.568, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8136, mae:0.6300 
epoch : 100 / 100  :  2022-05-29 04:44:16
train
[100,     0] loss: 0.006, The best rmse/mae: 0.794178 / 0.615951
[100,   100] loss: 0.560, The best rmse/mae: 0.794178 / 0.615951
test
rmse: 0.8064, mae:0.6206